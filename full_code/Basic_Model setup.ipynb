{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d8669c",
   "metadata": {},
   "source": [
    "# MODEL CODE - UNIDIRECTIONAL RNN WITH LSTM CELLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231a6d1",
   "metadata": {},
   "source": [
    "### Importing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a0eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from rdkit import Chem\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c23a7d",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01fe7ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.1 GiB for an array with shape (2372675, 1) and data type <U2045",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-44be82729df8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ChEMBL33.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\8qc00\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   2208\u001b[0m                         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mttype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2209\u001b[0m             \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2210\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2211\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0musemask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2212\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 18.1 GiB for an array with shape (2372675, 1) and data type <U2045"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt('ChEMBL33.csv',dtype='U')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd4677",
   "metadata": {},
   "source": [
    "### Visualising random structures in ChEMBL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871575dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(data[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(data[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42381c6a",
   "metadata": {},
   "source": [
    "### Removing unwanted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51677ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_data(filename_in='', filename_out='',  invalid=True, duplicates=True, salts=True, stereochem=True, canonicalize=True):\n",
    "    \"\"\"Pre-processing of SMILES based on the user-defined parameters\n",
    "            :param filename_in     path to the file containing the SMILES to pretreat (SMILES only) -- default = ChEMBL\n",
    "            :param filename_out    path for file export -- default = ../data/\n",
    "            :param invalid         if True (default), removes invalid SMILES\n",
    "            :param duplicates      if True (default), removes duplicates\n",
    "            :param salts           if True (default), removes salts\n",
    "            :param stereochem      if True (default), removes stereochemistry\n",
    "            :param canonicalize    if True (default), produces canonical SMILES\n",
    "    \"\"\"\n",
    "\n",
    "    from preprocessor import Preprocessor\n",
    "    p = Preprocessor(filename_in)\n",
    "\n",
    "    print(Fore.GREEN + 'Pre-processing of \"' + filename_in + '\" started.')\n",
    "\n",
    "\n",
    "    # user-defined pretreatment\n",
    "    if invalid:\n",
    "        p.remove_not_valid()\n",
    "        print(Fore.GREEN + ' invalid SMILES - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if duplicates:\n",
    "        p.remove_duplicates()\n",
    "        print(Fore.GREEN + ' duplicate SMILES - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if salts:\n",
    "        p.remove_salts()\n",
    "        print(Fore.GREEN + ' salts - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if stereochem:\n",
    "        p.remove_stereochem()\n",
    "        print(Fore.GREEN + ' stereochemistry - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if canonicalize:\n",
    "        p.canonicalize()\n",
    "        print(Fore.GREEN + ' canonicalized SMILES.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    #save data to file\n",
    "    p.save_data(filename_out)\n",
    "\n",
    "    data = p.get_data()\n",
    "    print(data[:5])\n",
    "\n",
    "\n",
    "remove_unwanted_data(filename_in='ChEMBL33', filename_out='preprocessed_ChEMBL33.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db501cbe",
   "metadata": {},
   "source": [
    "### Creating dictionary from preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825f5d7",
   "metadata": {},
   "source": [
    "In this part a dictionary is made that maps each unique character to an unique integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022bf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = np.genfromtxt('preprocessed_ChEMBL33.csv',dtype='U')\n",
    "\n",
    "unique_chars = sorted(list(OrderedDict.fromkeys(chain.from_iterable()))) #list of all unique characters found in SMILES data\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unique_chars)) #dict from character to integer. \n",
    "int_to_char = dict((i, c) for i, c in enumerate(unique_chars)) #dict from integer to character.\n",
    "\n",
    "\n",
    "#Addition of end and begin characters in dictionaries.\n",
    "char_to_int.update({\"E\" : len(char_to_int)})\n",
    "char_to_int.update({\"G\" : len(char_to_int)})\n",
    "\n",
    "int_to_char.update({len(int_to_char) : \"E\"})\n",
    "int_to_char.update({len(int_to_char) : \"G\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766eee2",
   "metadata": {},
   "source": [
    "### Creating subsets from preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c62f39",
   "metadata": {},
   "source": [
    "Method is from Skinnider paper, but adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_splitting(datafile, num_subsets=6, step_size=25000): \n",
    "    sample_sizes = [50000 + (i * step_size) for i in range(num_subsets)]\n",
    "    \n",
    "    #remove all subset sizes > than datafile size, as we cannot generate these\n",
    "    sample_sizes = []\n",
    "    for size in sample_sizes:\n",
    "        if size < len(datafile):\n",
    "            sample_sizes.append(size)\n",
    "    return sample_sizes\n",
    "    \n",
    "groups = subset_splitting(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e5df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_creation(datafile, group_num=1, groups):\n",
    "    df = pd.DataFrame(datafile)\n",
    "    if group_num < len(groups):\n",
    "        subset_size = groups[group_num]\n",
    "        dataset = df.sample(subset_size, replace=True, random_state=42)\n",
    "        return dataset\n",
    "    else:\n",
    "        print(\"Invalid group_num\")\n",
    "\n",
    "#As input give which subset you want to generate. \n",
    "random_subset = subset_creation(train_data, group_num=1, groups) #returns dataset from bigger file containing x amount of SMILES which are randomly sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d904c",
   "metadata": {},
   "source": [
    "### Lipinski descriptor distributions for created subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be55cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Lipinski descriptors\n",
    "def lipinski_descriptors(smiles):\n",
    "    \"\"\"Calculates the descriptors that are within the Lipinski rule of 5 and returns them as a dictionary\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    hbd = Descriptors.NumHDonors(mol)\n",
    "    hba = Descriptors.NumHAcceptors(mol)\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    return {'Hydrogen Bond Donors': hbd, 'Hydrogen Bond Acceptors': hba, 'Molecular Weight': mw, 'LogP': logp}\n",
    "\n",
    "def visualisation_distributions(data, output_filename):\n",
    "    descriptor_list = [] #by creating list of dictionaries, we can easily set up dataframe in panda. \n",
    "    for smiles in data['canonical_smiles']:\n",
    "        descriptor_list.append(lipinski_descriptors(smiles))\n",
    "    data_with_descriptors = pd.concat([data, pd.DataFrame(descriptor_list)], axis=1)\n",
    "    \n",
    "    descriptor_columns = list(data_with_descriptors.columns)[1:]\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.suptitle('Density Distributions of Lipinski Descriptors', fontsize=16, fontweight='bold')\n",
    "    for i, descriptor in enumerate(descriptor_columns, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.kdeplot(data_with_descriptors[descriptor], fill=True, color=f'C{i}', lw=2)\n",
    "        plt.xlabel(descriptor)\n",
    "        plt.ylabel('Density')\n",
    "    \n",
    "    data_with_descriptors.to_csv(output_filename, index=False)\n",
    "\n",
    "output_filename = 'example_smiles_descr.csv'\n",
    "visualisation_distributions(random_subset, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c50f90",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e77a9b",
   "metadata": {},
   "source": [
    "The encoder makes a 3D Numpy array which contains 2D one-hot encoded representations of the SMILES. This representation encodes each character in one-hot representation and adds 'endchar' symbol. It then returns two arrays; one for input sequence and the other for corresponding output.\n",
    "\n",
    "The decoder is used to convert a 3D numpy array of 2D one-hot encoded sequences, to a list of SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(data, char_to_int, max_seq):\n",
    "    \"\"\"\n",
    "    Encodes SMILES sequences into one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Array containing SMILES sequences.\n",
    "        char_to_int (dict): Mapping from characters to integer indices.\n",
    "        max_seq (int): Length of the longest sequence in the dataset for padding.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing input and output one-hot encoded arrays.\n",
    "    \"\"\"\n",
    "    #3D array to store one-hot encoded sequences\n",
    "    one_hot =  np.zeros((data.shape[0], embed+2, len(char_to_int)),dtype=np.int8) #embedded+2 to account for begin and end characters\n",
    "    \n",
    "    #loop through each SMILES sequence in data set and convert.\n",
    "    for i,smile in enumerate(data):\n",
    "        one_hot[i,0,char_to_int[\"G\"]] = 1\n",
    "        for j,c in enumerate(smile):\n",
    "            one_hot[i,j+1,char_to_int[c]] = 1\n",
    "        one_hot[i,len(smile)+1,char_to_int[\"E\"]] = 1\n",
    "        \n",
    "    #return input+output arrays\n",
    "    return one_hot[:,0:-1,:], one_hot[:,1:,:]\n",
    "\n",
    "def decoder(array_data, int_to_char):\n",
    "    \"\"\"\n",
    "    Decodes one-hot encoded sequences into SMILES strings.\n",
    "\n",
    "    Parameters:\n",
    "        array_data (numpy.ndarray): 3D array with 2D one-hot encoded array to be decoded.\n",
    "        int_to_char (dict): Mapping from integer indices to characters.\n",
    "\n",
    "    Returns:\n",
    "        list: list of decoded SMILES strings.\n",
    "    \"\"\"\n",
    "    gen_mol = []\n",
    "    \n",
    "    #iterates through each sequence in one-hot encoded array\n",
    "    for i in range(array_data.shape[0]):\n",
    "        decoded_seq = \"\"\n",
    "        for j in range(array_data.shape[1]):\n",
    "            char_index = np.argmax(array_data[i, j, :])\n",
    "            decoded_char = int_to_char[char_index]\n",
    "            \n",
    "            #append char to decoded_seq untill it is end char.\n",
    "            if decoded_char != \"E\":\n",
    "                decoded_seq += decoded_char\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    #append the fully decoded sequence to the list\n",
    "    gen_mol.append(decoded_seq)\n",
    "    \n",
    "    return gen_mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e57632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters\n",
    "embed = max([len(seq) for seq in train_data])\n",
    "\n",
    "#X: contains all characters except last of sequences & Y: contains all characters except first of sequences\n",
    "X, Y = encoder(train_data, char_to_int, embed) \n",
    "X, Y = shuffle(X, Y) #shuffled for randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fb64b",
   "metadata": {},
   "source": [
    "### RNN model with LSTM cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9c539f",
   "metadata": {},
   "source": [
    "Model is built with two LSTM layers with dropout for sequence processing. Batch normalization is applied to input and intermediate layers to enhance training stability. We also use a softmax activation function in the output layer. The model structure is mimicked to be like that of the MORET PAPER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f24802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#batch normalization for input data normalization\n",
    "model.add(BatchNormalization(input_shape=(None, X.shape[2])))\n",
    "\n",
    "model.add(LSTM(1024, return_sequences = True))\n",
    "model.add(Dropout(0.25)) #to prevent overfitting\n",
    "\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#normalization for regularization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Dense layer with softmax activation for output prediction\n",
    "model.add(Dense(Y.shape[-1], activation='softmax'))\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86b8c0",
   "metadata": {},
   "source": [
    "We then compile model using cross-entropy as loss function with Adam optimizer with learning rate of 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#set parameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Adam optimizer is used as it is popular for optimizing neural networks. \n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e78898",
   "metadata": {},
   "source": [
    "We then train the model and employ Early stopping to monitor trainin loss and minimize overfitting. The training is performed for a maximum of x epochs with a batch size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping #, ModelCheckpoint, ReduceLROnPlateau (willen we dit nog gebruiken?)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#set parameters\n",
    "validation_fraction = 0.2\n",
    "patience = 5\n",
    "epochs = 100 #epochs in Moret is 10. 40 for transfer learning. \n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "print(f\"Original X Data Set Length: {len(X)}\")\n",
    "print(f\"Original Y Data Set Length: {len(X)}\")\n",
    "\n",
    "#split train and validation data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=validation_fraction, random_state=42)\n",
    "\n",
    "print(f\"X Training Set Length: {len(X_train)}\")\n",
    "print(f\"Y Training Set Length: {len(Y_train)}\")\n",
    "print(f\"X Validation Set Length: {len(X_val)}\")\n",
    "print(f\"Y Validation Set Length: {len(Y_val)}\")\n",
    "\n",
    "#You monitor the loss of the model. Training will stop if validation loss does not increase for 5 epochs. \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, min_delta=0.01)\n",
    "\n",
    "#save training history\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size = 256, callbacks=[early_stopping], validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738697e",
   "metadata": {},
   "source": [
    "### Plot cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plotting the training loss over epochs\n",
    "plt.plot(history.history[\"loss\"], '-o', label=\"Loss\")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb88a41",
   "metadata": {},
   "source": [
    "### Sampling from trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcd66c",
   "metadata": {},
   "source": [
    "Two functions are defined. One does the temperature sampling from the predictions from the trained model. The other function is responsible for actual generating of the sampled SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters\n",
    "max_length = 30\n",
    "temperature = 0.7\n",
    "num_samples = embed\n",
    "\n",
    "\n",
    "def sampling(model, start_char=\"G\", max_length=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate SMILES sequences using trained model.\n",
    "\n",
    "    Parameters:\n",
    "        model (keras.Model): The trained model for SMILES generation.\n",
    "        start_char (str): The starting character for SMILES generation.\n",
    "        max_length (int): Maximum length of the generated SMILES sequence.\n",
    "        temperature (float): Controls the randomness of sampling.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: One-hot encoded array representing the generated SMILES sequence.\n",
    "    \"\"\"\n",
    "    input_sequence = np.zeros((1, 1, len(char_to_int)))\n",
    "    input_sequence[0, 0, char_to_int[start_char]] = 1\n",
    "    \n",
    "    #generate SMILES sequence in one-hot encoding format. \n",
    "    for _ in range(max_length):\n",
    "        #temperature sampling\n",
    "        predictions = model.predict(input_sequence)\n",
    "        predictions = np.log(predictions) / temperature\n",
    "        exp_predictions = np.exp(predictions)\n",
    "        normalized_predictions = exp_predictions / np.sum(exp_predictions)\n",
    "\n",
    "        #Sampling the next char using temperature sampling.\n",
    "        pred_char_index = np.random.choice(len(char_to_int), p=normalized_predictions[0, -1, :] / np.sum(normalized_predictions[0, -1, :]))\n",
    "        pred_char = int_to_char[pred_char_index]\n",
    "        \n",
    "        #if E is predicted, break\n",
    "        if pred_char == \"E\":\n",
    "            break\n",
    "        else:\n",
    "            #add next char to 2D array. \n",
    "            input_vector = np.zeros((1, 1, len(char_to_int)))\n",
    "            input_vector[0, 0, char_to_int[pred_char]] = 1\n",
    "            input_sequence = np.concatenate([input_sequence, input_vector], axis=1)\n",
    "            \n",
    "    return input_sequence\n",
    "\n",
    "\n",
    "def generate_smiles(model, num_samples=1000, max_length=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate multiple SMILES sequences using sampling function\n",
    "\n",
    "    Parameters:\n",
    "        model (keras.Model): The trained model for SMILES generation.\n",
    "        num_samples (int): Number of SMILES sequences to generate.\n",
    "        max_length (int): Maximum length of each generated SMILES sequence.\n",
    "        temperature (float): Controls the randomness of character sampling.\n",
    "\n",
    "    Returns:\n",
    "        list: List of generated SMILES sequences.\n",
    "    \"\"\"\n",
    "    generated_sequences = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        #sample\n",
    "        one_hot = sampling(model, start_char=\"G\", max_length=max_length, temperature=temperature)\n",
    "        \n",
    "        #decode one-hot sequence into SMILES string\n",
    "        generated_sequence = decoder(one_hot, int_to_char)\n",
    "        \n",
    "        #append generated SMILES string to list\n",
    "        generated_sequences.append(generated_sequence)\n",
    "        \n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "x = generate_smiles(model, num_samples=50, max_length=30, temperature=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
