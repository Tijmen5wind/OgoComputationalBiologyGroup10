{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d8669c",
   "metadata": {},
   "source": [
    "# MODEL CODE - UNIDIRECTIONAL RNN WITH LSTM CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9c767d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from rdkit import Chem\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c01fe7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ['CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1',\n",
    "       'COc1ccc(-c2cc(=O)c3c(O)c(OC)c(OC)cc3o2)cc1O',\n",
    "       'CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(F)ccc1-2',\n",
    "       'Clc1ccccc1-c1nc(-c2ccncc2)no1',\n",
    "       'CC(C)(Oc1ccc(Cl)cc1)C(=O)OCc1cccc(CO)n1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a99f534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1',\n",
       "       'COc1ccc(-c2cc(=O)c3c(O)c(OC)c(OC)cc3o2)cc1O',\n",
       "       'CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(F)ccc1-2',\n",
       "       'Clc1ccccc1-c1nc(-c2ccncc2)no1',\n",
       "       'CC(C)(Oc1ccc(Cl)cc1)C(=O)OCc1cccc(CO)n1'], dtype='<U43')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4dc0a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x29734fa5300>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chem.MolFromSmiles(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "022bf3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(', ')', '-', '1', '2', '3', '=', 'C', 'F', 'N', 'O', 'c', 'l', 'n', 'o']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating mapping for each char to integer, also mapping for the E (end) is manually inserted into the dictionaries.\n",
    "unique_chars = sorted(list(OrderedDict.fromkeys(chain.from_iterable(train_data))))\n",
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "158da864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 0,\n",
       " ')': 1,\n",
       " '-': 2,\n",
       " '1': 3,\n",
       " '2': 4,\n",
       " '3': 5,\n",
       " '=': 6,\n",
       " 'C': 7,\n",
       " 'F': 8,\n",
       " 'N': 9,\n",
       " 'O': 10,\n",
       " 'c': 11,\n",
       " 'l': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'E': 15}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maps each unique character as int\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unique_chars)) #from character to int\n",
    "int_to_char = dict((i, c) for i, c in enumerate(unique_chars)) #from int to character\n",
    "\n",
    "#add E to both dict for padding\n",
    "char_to_int.update({\"E\" : len(char_to_int)})\n",
    "int_to_char.update({len(int_to_char) : \"E\"})\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "937e33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(data, int_to_char, char_to_int, embed):\n",
    "    \n",
    "    one_hot =  np.zeros((data.shape[0], embed+1, len(char_to_int)),dtype=np.int8)\n",
    "    for i,smile in enumerate(data):\n",
    "        #encode the chars\n",
    "        for j,c in enumerate(smile):\n",
    "            one_hot[i,j,char_to_int[c]] = 1\n",
    "        #Encode endchar\n",
    "        one_hot[i,len(smile):,char_to_int[\"E\"]] = 1\n",
    "    #Return two, one for input and the other for output\n",
    "    return one_hot[:,0:-1,:], one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6246ed8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = max([len(seq) for seq in train_data]) #langste, zodat je weet hoeveel je moet padden. \n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95e57632",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = gen_data(train_data, int_to_char, char_to_int, embed)\n",
    "X, Y = shuffle(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66f24802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_6 (Batch (None, None, 16)          64        \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, None, 512)         1083392   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, None, 256)         787456    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, None, 16)          4112      \n",
      "=================================================================\n",
      "Total params: 1,876,048\n",
      "Trainable params: 1,875,504\n",
      "Non-trainable params: 544\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model (simple 2 layer LSTM)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(None, X.shape[2])))\n",
    "\n",
    "model.add(LSTM(512, input_shape=(None, X.shape[2]), return_sequences = True))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(Y.shape[-1], activation='softmax'))\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1836aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e7a6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.2632 - accuracy: 0.9209\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.2440 - accuracy: 0.9349\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.2221 - accuracy: 0.9349\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2452 - accuracy: 0.9163\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.2321 - accuracy: 0.9302\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.2042 - accuracy: 0.9395\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.1819 - accuracy: 0.9628\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.1991 - accuracy: 0.9442\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.1687 - accuracy: 0.9581\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.1860 - accuracy: 0.9395\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "#if validation loss does not increase for 3 epochs. \n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X, Y, epochs = 10, batch_size = 256, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit_generator(datagenerator(X_train, y_train, patch_size, patches_per_im, batch_size),\n",
    "                              validation_data=(val_images, val_segmentations),\n",
    "                              steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=2,\n",
    "                              callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
