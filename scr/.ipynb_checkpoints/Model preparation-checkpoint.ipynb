{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the necessary packages and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:06:31.254936400Z",
     "start_time": "2023-12-02T11:06:31.252941900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a subset of SMILES so its faster to test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:29:05.598695700Z",
     "start_time": "2023-12-02T10:29:01.841466200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 SMILES to subset_smiles.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_smiles(csv_file_path, output_csv_file, limit=1000):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract the first 'limit' number of SMILES\n",
    "    subset = data.iloc[:limit]\n",
    "\n",
    "    # Save the subset to a new CSV file\n",
    "    subset.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    print(f\"Saved {limit} SMILES to {output_csv_file}\")\n",
    "\n",
    "# Usage\n",
    "csv_file_path = 'ChEMBL33.csv'\n",
    "output_csv_file = 'subset_smiles.csv'\n",
    "extract_smiles(csv_file_path, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted data and cannonicalizing\n",
    "This involves removing invalid data, duplicates, salts, stereochems. Finally it cannonicalizes the smiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:31:44.724952200Z",
     "start_time": "2023-12-02T10:31:43.236931600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mPre-processing of \"subset_smiles\" started.\n",
      "\u001b[32m invalid SMILES - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m duplicate SMILES - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m salts - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m stereochemistry - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m canonicalized SMILES.\n",
      "\u001b[0m\n",
      "['Brc1cccc(Nc2ncnc3ccncc23)c1NCCN1CCOCC1' 'C#CC(C)(O)CC'\n",
      " 'C#CC1CCC(C#N)N1C(=O)CNC12CC3CC(CC(O)(C3)C1)C2'\n",
      " 'C#CC=CCCCCCCCCCCC=CCCCCCC'\n",
      " 'C#CC=CCCCCCCCCCCCCCCC=CCCCCC(O)C=CCCCC#CC(O)C#CCCCCCCC=CC(O)C#C']\n"
     ]
    }
   ],
   "source": [
    "def remove_unwanted_data(filename_in='', filename_out='',  invalid=True, duplicates=True, salts=True, stereochem=True, canonicalize=True):\n",
    "    \"\"\"Pre-processing of SMILES based on the user-defined parameters\n",
    "            :param filename_in     path to the file containing the SMILES to pretreat (SMILES only) -- default = ChEMBL\n",
    "            :param filename_out    path for file export -- default = ../data/\n",
    "            :param invalid         if True (default), removes invalid SMILES\n",
    "            :param duplicates      if True (default), removes duplicates\n",
    "            :param salts           if True (default), removes salts\n",
    "            :param stereochem      if True (default), removes stereochemistry\n",
    "            :param canonicalize    if True (default), produces canonical SMILES\n",
    "    \"\"\"\n",
    "\n",
    "    from preprocessor import Preprocessor\n",
    "    p = Preprocessor(filename_in)\n",
    "\n",
    "    print(Fore.GREEN + 'Pre-processing of \"' + filename_in + '\" started.')\n",
    "\n",
    "\n",
    "    # user-defined pretreatment\n",
    "    if invalid:\n",
    "        p.remove_not_valid()\n",
    "        print(Fore.GREEN + ' invalid SMILES - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if duplicates:\n",
    "        p.remove_duplicates()\n",
    "        print(Fore.GREEN + ' duplicate SMILES - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if salts:\n",
    "        p.remove_salts()\n",
    "        print(Fore.GREEN + ' salts - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if stereochem:\n",
    "        p.remove_stereochem()\n",
    "        print(Fore.GREEN + ' stereochemistry - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if canonicalize:\n",
    "        p.canonicalize()\n",
    "        print(Fore.GREEN + ' canonicalized SMILES.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    #save data to file\n",
    "    p.save_data(filename_out)\n",
    "\n",
    "    data = p.get_data()\n",
    "    print(data[:5])\n",
    "\n",
    "\n",
    "remove_unwanted_data(filename_in='subset_smiles', filename_out='testPreprocessed_Data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Lipinski rule of 5\n",
    "Here we will calculate the descriptors that are within the rule of 5: HBD, HBA, MW and LogP. This will be used to compare the subsets of data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocessed_ChEMBL33.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-da7539236a96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the preprocessed data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'preprocessed_ChEMBL33.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Function to calculate Lipinski descriptors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlipinski_descriptors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\8qc00\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\8qc00\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\8qc00\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\8qc00\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\8qc00\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocessed_ChEMBL33.csv'"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "data = pd.read_csv('preprocessed_ChEMBL33.csv')\n",
    "\n",
    "# Function to calculate Lipinski descriptors\n",
    "def lipinski_descriptors(smiles):\n",
    "    \"\"\"Calculates the descriptors that are within the Lipinski rule of 5 and adds them as separate columns to the data.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    hbd = Descriptors.NumHDonors(mol)\n",
    "    hba = Descriptors.NumHAcceptors(mol)\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    return hbd, hba, mw, logp\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "data[['HBD', 'HBA', 'MW', 'LogP']] = data.apply(\n",
    "    lambda row: lipinski_descriptors(row['standardized_smiles']), axis=1, result_type='expand')\n",
    "\n",
    "# Save the data with descriptors\n",
    "data.to_csv('ChEMBL33_with_descriptors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:07:16.672563200Z",
     "start_time": "2023-12-02T11:07:16.594742100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encode_smiles(csv_file_path):\n",
    "    \"\"\"Converts SMILES to one-hot encoded sequences.\"\"\"\n",
    "    Smiles_data = pd.read_csv(csv_file_path)\n",
    "    smiles_list = Smiles_data.iloc[:, 0].tolist()\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(smiles_list)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(smiles_list)\n",
    "    one_hot = tokenizer.sequences_to_matrix(sequences, mode='binary')\n",
    "\n",
    "    return one_hot, tokenizer\n",
    "\n",
    "csv_file_path = 'testPreprocessed_Data.csv'\n",
    "one_hot_encoded_smiles, tokenizer = one_hot_encode_smiles(csv_file_path)\n",
    "\n",
    "one_hot_encoded_smiles[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:07:35.061378300Z",
     "start_time": "2023-12-02T11:07:35.057388900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_smiles_sequences(encoded_sequences, max_length=None):\n",
    "    \"\"\"Ensures all encoded sequences are of the same length by padding shorter sequences with zeros. You can choose a suitable maximum length or leave max_length set to None to let the function determine it based on the longest sequence in the data.\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in encoded_sequences)\n",
    "\n",
    "    padded_sequences = pad_sequences(encoded_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "padded_smiles = pad_smiles_sequences(one_hot_encoded_smiles, max_length=None)\n",
    "padded_smiles[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method from skinnider paper, but adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELE BASALE CODE, MOET NOG VERBETERD WORDEN.\n",
    "train_data = np.genfromtxt('testPreprocessed_Data.csv', dtype=\"U\")\n",
    "\n",
    "def subset_splitting(datafile, num_subsets=6, step_size=25000): \n",
    "    sample_sizes = [50000 + (i * step_size) for i in range(num_subsets)]\n",
    "    \n",
    "    #remove all subset sizes > than datafile size, as we cannot generate these\n",
    "    #sample_sizes = [size for size in sample_sizes if size < len(datafile)]\n",
    "    return sample_sizes\n",
    "    \n",
    "groups = subset_splitting(train_data)\n",
    "\n",
    "def subset_creation(datafile, group_num, groups):\n",
    "    df = pd.DataFrame(datafile)\n",
    "    if group_num < len(groups):\n",
    "        subset_size = groups[group_num]\n",
    "        dataset = df.sample(subset_size, replace=True, random_state=42)\n",
    "        return dataset\n",
    "    else:\n",
    "        print(\"Invalid group_num\")\n",
    "\n",
    "random_subset = subset_creation(train_data, 1, groups) #returns dataset from bigger file containing x amount of SMILES which are randomly sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare subsets based on rule of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split each subset into test/train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set Length: 992\n",
      "Updated Training Set Length: 793\n",
      "Validation Set Length: 199\n"
     ]
    }
   ],
   "source": [
    "validation_fraction = 0.2\n",
    "\n",
    "# Print the length of the original data set\n",
    "print(f\"Original Data Set Length: {len(train_data)}\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "training, val_data = train_test_split(train_data, test_size=validation_fraction, random_state=42)\n",
    "\n",
    "# Print the length of the updated training set\n",
    "print(f\"Updated Training Set Length: {len(training)}\")\n",
    "\n",
    "# Print the length of the validation set\n",
    "print(f\"Validation Set Length: {len(val_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
