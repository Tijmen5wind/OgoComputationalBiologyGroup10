{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the necessary packages and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:06:31.254936400Z",
     "start_time": "2023-12-02T11:06:31.252941900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a subset of SMILES so its faster to test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:29:05.598695700Z",
     "start_time": "2023-12-02T10:29:01.841466200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 SMILES to subset_smiles.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_smiles(csv_file_path, output_csv_file, limit=1000):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract the first 'limit' number of SMILES\n",
    "    subset = data.iloc[:limit]\n",
    "\n",
    "    # Save the subset to a new CSV file\n",
    "    subset.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    print(f\"Saved {limit} SMILES to {output_csv_file}\")\n",
    "\n",
    "# Usage\n",
    "csv_file_path = 'ChEMBL33.csv'\n",
    "output_csv_file = 'subset_smiles.csv'\n",
    "extract_smiles(csv_file_path, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted data and cannonicalizing\n",
    "This involves removing invalid data, duplicates, salts, stereochems. Finally it cannonicalizes the smiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:31:44.724952200Z",
     "start_time": "2023-12-02T10:31:43.236931600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mPre-processing of \"subset_smiles\" started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:31:43] SMILES Parse Error: syntax error while parsing: canonical_smiles\n",
      "[11:31:43] SMILES Parse Error: Failed parsing SMILES 'canonical_smiles' for input: 'canonical_smiles'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m invalid SMILES - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m duplicate SMILES - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m salts - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m stereochemistry - removed.\n",
      "\u001b[0m\n",
      "\u001b[32m canonicalized SMILES.\n",
      "\u001b[0m\n",
      "['Brc1cccc(Nc2ncnc3ccncc23)c1NCCN1CCOCC1' 'C#CC(C)(O)CC'\n",
      " 'C#CC1CCC(C#N)N1C(=O)CNC12CC3CC(CC(O)(C3)C1)C2'\n",
      " 'C#CC=CCCCCCCCCCCC=CCCCCCC'\n",
      " 'C#CC=CCCCCCCCCCCCCCCC=CCCCCC(O)C=CCCCC#CC(O)C#CCCCCCCC=CC(O)C#C']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_unwanted_data(filename_in='', filename_out='',  invalid=True, duplicates=True, salts=True, stereochem=True, canonicalize=True):\n",
    "    \"\"\"Pre-processing of SMILES based on the user-defined parameters\n",
    "            :param filename_in     path to the file containing the SMILES to pretreat (SMILES only) -- default = ChEMBL\n",
    "            :param filename_out    path for file export -- default = ../data/\n",
    "            :param invalid         if True (default), removes invalid SMILES\n",
    "            :param duplicates      if True (default), removes duplicates\n",
    "            :param salts           if True (default), removes salts\n",
    "            :param stereochem      if True (default), removes stereochemistry\n",
    "            :param canonicalize    if True (default), produces canonical SMILES\n",
    "    \"\"\"\n",
    "\n",
    "    from preprocessor import Preprocessor\n",
    "    p = Preprocessor(filename_in)\n",
    "\n",
    "    print(Fore.GREEN + 'Pre-processing of \"' + filename_in + '\" started.')\n",
    "\n",
    "\n",
    "    # user-defined pretreatment\n",
    "    if invalid:\n",
    "        p.remove_not_valid()\n",
    "        print(Fore.GREEN + ' invalid SMILES - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if duplicates:\n",
    "        p.remove_duplicates()\n",
    "        print(Fore.GREEN + ' duplicate SMILES - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if salts:\n",
    "        p.remove_salts()\n",
    "        print(Fore.GREEN + ' salts - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if stereochem:\n",
    "        p.remove_stereochem()\n",
    "        print(Fore.GREEN + ' stereochemistry - removed.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    if canonicalize:\n",
    "        p.canonicalize()\n",
    "        print(Fore.GREEN + ' canonicalized SMILES.')\n",
    "        print(Style.RESET_ALL)\n",
    "\n",
    "    #save data to file\n",
    "    p.save_data(filename_out)\n",
    "\n",
    "    data = p.get_data()\n",
    "    print(data[:5])\n",
    "\n",
    "\n",
    "remove_unwanted_data(filename_in='subset_smiles', filename_out='testPreprocessed_Data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Lipinski rule of 5\n",
    "Here we will calculate the descriptors that are within the rule of 5: HBD, HBA, MW and LogP. This will be used to compare the subsets of data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data = pd.read_csv('preprocessed_ChEMBL33.csv')\n",
    "\n",
    "# Function to calculate Lipinski descriptors\n",
    "def lipinski_descriptors(smiles):\n",
    "    \"\"\"Calculates the descriptors that are within the Lipinski rule of 5 and adds them as separate columns to the data.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    hbd = Descriptors.NumHDonors(mol)\n",
    "    hba = Descriptors.NumHAcceptors(mol)\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    return hbd, hba, mw, logp\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "data[['HBD', 'HBA', 'MW', 'LogP']] = data.apply(\n",
    "    lambda row: lipinski_descriptors(row['standardized_smiles']), axis=1, result_type='expand')\n",
    "\n",
    "# Save the data with descriptors\n",
    "data.to_csv('ChEMBL33_with_descriptors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:07:16.672563200Z",
     "start_time": "2023-12-02T11:07:16.594742100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encode_smiles(csv_file_path):\n",
    "    \"\"\"Converts SMILES to one-hot encoded sequences.\"\"\"\n",
    "    Smiles_data = pd.read_csv(csv_file_path)\n",
    "    smiles_list = Smiles_data.iloc[:, 0].tolist()\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(smiles_list)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(smiles_list)\n",
    "    one_hot = tokenizer.sequences_to_matrix(sequences, mode='binary')\n",
    "\n",
    "    return one_hot, tokenizer\n",
    "\n",
    "csv_file_path = 'testPreprocessed_Data.csv'\n",
    "one_hot_encoded_smiles, tokenizer = one_hot_encode_smiles(csv_file_path)\n",
    "\n",
    "one_hot_encoded_smiles[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:07:35.061378300Z",
     "start_time": "2023-12-02T11:07:35.057388900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_smiles_sequences(encoded_sequences, max_length=None):\n",
    "    \"\"\"Ensures all encoded sequences are of the same length by padding shorter sequences with zeros. You can choose a suitable maximum length or leave max_length set to None to let the function determine it based on the longest sequence in the data.\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in encoded_sequences)\n",
    "\n",
    "    padded_sequences = pad_sequences(encoded_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "padded_smiles = pad_smiles_sequences(one_hot_encoded_smiles, max_length=None)\n",
    "padded_smiles[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare subsets based on rule of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split each subset into test/train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
